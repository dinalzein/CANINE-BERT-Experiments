{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Testing Canine on SQuAD dataset"
      ],
      "metadata": {
        "id": "GNCSr3h0MCWJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notebook adapted from [Hugging faceâ€™s QA guide](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb)"
      ],
      "metadata": {
        "id": "Dt0KxwiN2ZPI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setup"
      ],
      "metadata": {
        "id": "xep1R_scpWKj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOsHUjgdIrIW"
      },
      "outputs": [],
      "source": [
        "!pip install datasets transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "cbjhlt8PfuvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVvslsfMIrIh"
      },
      "outputs": [],
      "source": [
        "model_checkpoint = \"google/canine-s\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whPRbBNbIrIl"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Loading the dataset"
      ],
      "metadata": {
        "id": "017C0O0TpvSJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IreSlFmlIrIm"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, load_metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_AY1ATSIrIq"
      },
      "outputs": [],
      "source": [
        "datasets = load_dataset(\"squad\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Display the dataset:**"
      ],
      "metadata": {
        "id": "2Knapoy0MrRw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3j8APAoIrI3"
      },
      "outputs": [],
      "source": [
        "from datasets import ClassLabel, Sequence\n",
        "import random\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def show_random_elements(dataset, num_examples=10):\n",
        "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
        "    picks = []\n",
        "    for _ in range(num_examples):\n",
        "        pick = random.randint(0, len(dataset)-1)\n",
        "        while pick in picks:\n",
        "            pick = random.randint(0, len(dataset)-1)\n",
        "        picks.append(pick)\n",
        "    \n",
        "    df = pd.DataFrame(dataset[picks])\n",
        "    for column, typ in dataset.features.items():\n",
        "        if isinstance(typ, ClassLabel):\n",
        "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
        "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
        "            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n",
        "    display(HTML(df.to_html()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZy5tRB_IrI7",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "show_random_elements(datasets[\"train\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9qywopnIrJH"
      },
      "source": [
        "## Preprocessing the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXNLu_-nIrJI"
      },
      "outputs": [],
      "source": [
        "from transformers import CanineTokenizer, CanineForQuestionAnswering\n",
        "import transformers\n",
        "    \n",
        "tokenizer = CanineTokenizer.from_pretrained(\"google/canine-s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5R34mK4Z8t4J"
      },
      "outputs": [],
      "source": [
        "max_length = 2048 # The maximum length of a feature (question and context)\n",
        "doc_stride = 128 # The authorized overlap between two part of the context when splitting it is needed. #Not really needed here"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Check the output of the tokenizer:**"
      ],
      "metadata": {
        "id": "VPbA1BCtN74S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5hBlsrHIrJL"
      },
      "outputs": [],
      "source": [
        "tokenizer(\"What is your name?\", \"My name is Sylvain.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aY7EwP5M8t4K"
      },
      "outputs": [],
      "source": [
        "example = datasets[\"train\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfk7W0kK8t4Q"
      },
      "outputs": [],
      "source": [
        "tokenized_example = tokenizer(\n",
        "    example[\"question\"],\n",
        "    example[\"context\"],\n",
        "    max_length=max_length,\n",
        "    truncation=\"only_second\",\n",
        "    return_overflowing_tokens=True,\n",
        "    stride=doc_stride\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cY8fQihEQDOj"
      },
      "outputs": [],
      "source": [
        "tokenized_example.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Check the link between caracters and tokens position:**"
      ],
      "metadata": {
        "id": "52UJX4U3NU1-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIzP-5Fj8t4T"
      },
      "outputs": [],
      "source": [
        "answers = example[\"answers\"]\n",
        "start_char = answers[\"answer_start\"][0]\n",
        "end_char = start_char + len(answers[\"text\"][0])\n",
        "\n",
        "offset = len(example[\"question\"])+2  #+2 because of the 2 CLS tokens\n",
        "start_position, end_position = start_char+offset, end_char+offset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUIOlXWc8t4U"
      },
      "outputs": [],
      "source": [
        "print(example[\"context\"][start_char: end_char])\n",
        "print(tokenizer.decode(tokenized_example[\"input_ids\"][start_position: end_position]))\n",
        "print(answers[\"text\"][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Add the start and end token position of the answer in the text:**"
      ],
      "metadata": {
        "id": "hOTLQKAVNdBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pad_on_right = tokenizer.padding_side == \"right\""
      ],
      "metadata": {
        "id": "E7tMphjw3W26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YeC8OjtxMgvT"
      },
      "outputs": [],
      "source": [
        "def prepare_train_features(examples):\n",
        " \n",
        "  examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
        "\n",
        "  tokenized_examples = tokenizer(\n",
        "      examples[\"question\"],\n",
        "      examples[\"context\"],\n",
        "      truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
        "      max_length=max_length,\n",
        "      stride=doc_stride,\n",
        "      padding=\"max_length\",\n",
        "  )\n",
        "\n",
        "  tokenized_examples[\"start_positions\"] = []\n",
        "  tokenized_examples[\"end_positions\"] = []\n",
        "\n",
        "  for i in range(len(tokenized_examples[\"input_ids\"])):\n",
        "    answers = examples[\"answers\"][i]\n",
        "    start_char = answers[\"answer_start\"][0]\n",
        "    end_char = start_char + len(answers[\"text\"][0])\n",
        "\n",
        "    offset = len(examples[\"question\"][i])+2\n",
        "    start_position, end_position = start_char+offset, end_char+offset\n",
        "\n",
        "    tokenized_examples[\"start_positions\"].append(start_position)\n",
        "    tokenized_examples[\"end_positions\"].append(end_position)\n",
        "\n",
        "  return tokenized_examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9_1Gh-LMudu"
      },
      "outputs": [],
      "source": [
        "examples = datasets['train'][:5]\n",
        "print(examples[\"answers\"][:])\n",
        "features = prepare_train_features(examples)\n",
        "print(features[\"start_positions\"])\n",
        "for i in range(len(features[\"input_ids\"])):\n",
        "  print(tokenizer.decode(features[\"input_ids\"][i][features[\"start_positions\"][i]: features[\"end_positions\"][i]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDtsaJeVIrJT"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets = datasets.map(prepare_train_features, batched=True, remove_columns=datasets[\"train\"].column_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "545PP3o8IrJV"
      },
      "source": [
        "# Fine-tuning the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlqNaB8jIrJW"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
        "\n",
        "model = CanineForQuestionAnswering.from_pretrained(\"google/canine-s\")\n",
        "batch_size = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bliy8zgjIrJY"
      },
      "outputs": [],
      "source": [
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "args = TrainingArguments(\n",
        "    f\"{model_name}-finetuned-squad\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0.01,\n",
        "    push_to_hub=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cISCGZjj8t4Z"
      },
      "outputs": [],
      "source": [
        "from transformers import default_data_collator\n",
        "\n",
        "data_collator = default_data_collator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imY1oC3SIrJf"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNx5pyRlIrJh"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uk4s5Mht8t4b"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(\"test-squad-trained\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEvfTy4Q8t4b"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3021IEtg8t4b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "for batch in trainer.get_eval_dataloader():\n",
        "    break\n",
        "batch = {k: v.to(trainer.args.device) for k, v in batch.items()}\n",
        "with torch.no_grad():\n",
        "    output = trainer.model(**batch)\n",
        "output.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ka75VtXp8t4d"
      },
      "outputs": [],
      "source": [
        "n_best_size = 20"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenize the validation dataset:**"
      ],
      "metadata": {
        "id": "mq5eDXPyQMmS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-wMcPsWxbP8"
      },
      "outputs": [],
      "source": [
        "def prepare_validation_features(examples):\n",
        "    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
        "    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
        "    # left whitespace\n",
        "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
        "\n",
        "    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
        "    # in one example possible giving several features when a context is long, each of those features having a\n",
        "    # context that overlaps a bit the context of the previous feature.\n",
        "    tokenized_examples = tokenizer(\n",
        "        examples[\"question\" if pad_on_right else \"context\"],\n",
        "        examples[\"context\" if pad_on_right else \"question\"],\n",
        "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
        "        max_length=max_length,\n",
        "        stride=doc_stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    return tokenized_examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IrLwwkoU8t4e"
      },
      "outputs": [],
      "source": [
        "validation_features = datasets[\"validation\"].map(\n",
        "    prepare_validation_features,\n",
        "    batched=True,\n",
        "    remove_columns=datasets[\"validation\"].column_names\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Apply the model to the validation dataset:**"
      ],
      "metadata": {
        "id": "eggClEQPQWDN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziEWXYUi8t4g"
      },
      "outputs": [],
      "source": [
        "raw_predictions = trainer.predict(validation_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lo-cDQI38t4i"
      },
      "outputs": [],
      "source": [
        "validation_features.set_format(type=validation_features.format[\"type\"], columns=list(validation_features.features.keys()))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Check if the model works on one example:**"
      ],
      "metadata": {
        "id": "mm2UNJ_gPtTt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5yGxyDt8t4i"
      },
      "outputs": [],
      "source": [
        "max_answer_length = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHbJlSom9JHV"
      },
      "outputs": [],
      "source": [
        " datasets[\"validation\"][1][\"question\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3lZ6kGN8t4i"
      },
      "outputs": [],
      "source": [
        "#Get the best answers of the model for this exemple\n",
        "\n",
        "start_logits = output.start_logits[1].cpu().numpy()\n",
        "end_logits = output.end_logits[1].cpu().numpy()\n",
        "context = datasets[\"validation\"][1][\"context\"]\n",
        "\n",
        "# Gather the indices the best start/end logits:\n",
        "start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "valid_answers = []\n",
        "for start_index in start_indexes:\n",
        "    for end_index in end_indexes:\n",
        "        # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
        "        if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
        "            continue\n",
        "        if start_index <= end_index: # We need to refine that test to check the answer is inside the context\n",
        "            start_char = start_index - len(datasets[\"validation\"][1][\"question\"]) - 2\n",
        "            end_char = end_index - len(datasets[\"validation\"][1][\"question\"]) - 2\n",
        "            valid_answers.append(\n",
        "                {\n",
        "                    \"score\": start_logits[start_index] + end_logits[end_index],\n",
        "                    \"text\": context[start_char: end_char]\n",
        "                }\n",
        "            )\n",
        "\n",
        "valid_answers = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n",
        "valid_answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HO47vQFS8t4j"
      },
      "outputs": [],
      "source": [
        "datasets[\"validation\"][1][\"answers\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compute the answers of the questions:**"
      ],
      "metadata": {
        "id": "xzbsjUqMQAFW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0C8yT5L8t4j"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "\n",
        "examples = datasets[\"validation\"]\n",
        "features = validation_features\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uECDv_AmDHRF"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "#Get the predictions to text format\n",
        "def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n",
        "    all_start_logits, all_end_logits = raw_predictions\n",
        "\n",
        "    # The dictionaries we have to fill.\n",
        "    predictions = collections.OrderedDict()\n",
        "\n",
        "    # Let's loop over all the examples!\n",
        "    for example_index, example in enumerate(tqdm(examples)):\n",
        "        # Those are the indices of the features associated to the current example.\n",
        "        feature_indices = [example_index]\n",
        "\n",
        "        min_null_score = None # Only used if squad_v2 is True.\n",
        "        valid_answers = []\n",
        "        \n",
        "        context = example[\"context\"]\n",
        "        # Looping through all the features associated to the current example.\n",
        "        for feature_index in feature_indices:\n",
        "            # We grab the predictions of the model for this feature.\n",
        "            start_logits = all_start_logits[feature_index]\n",
        "            end_logits = all_end_logits[feature_index]\n",
        "\n",
        "            # Update minimum null prediction.\n",
        "            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
        "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
        "            if min_null_score is None or min_null_score < feature_null_score:\n",
        "                min_null_score = feature_null_score\n",
        "\n",
        "            # Go through all possibilities for the `n_best_size` greater start and end logits.\n",
        "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "            for start_index in start_indexes:\n",
        "                for end_index in end_indexes:\n",
        "\n",
        "                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
        "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
        "                        continue\n",
        "\n",
        "                    # Compute the characters position associated to the tokens\n",
        "                    offset = len(example[\"question\"]) + 2\n",
        "                    start_char = start_index - offset\n",
        "                    end_char = end_index - offset\n",
        "                    valid_answers.append(\n",
        "                        {\n",
        "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
        "                            \"text\": context[start_char: end_char]\n",
        "                        }\n",
        "                    )\n",
        "        \n",
        "        if len(valid_answers) > 0:\n",
        "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
        "        else:\n",
        "            # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n",
        "            # failure.\n",
        "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
        "        \n",
        "        predictions[example[\"id\"]] = best_answer[\"text\"]\n",
        "\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8LGzY1-8t4l"
      },
      "outputs": [],
      "source": [
        "final_predictions = postprocess_qa_predictions(datasets[\"validation\"], validation_features, raw_predictions.predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compute the metrics:**"
      ],
      "metadata": {
        "id": "QnqAwXqsRO2X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCK0R15q8t4l"
      },
      "outputs": [],
      "source": [
        "metric = load_metric(\"squad\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84VMEdHP8t4m"
      },
      "outputs": [],
      "source": [
        "formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in final_predictions.items()]\n",
        "references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in datasets[\"validation\"]]\n",
        "metric.compute(predictions=formatted_predictions, references=references)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "SQuAD_experiments.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}